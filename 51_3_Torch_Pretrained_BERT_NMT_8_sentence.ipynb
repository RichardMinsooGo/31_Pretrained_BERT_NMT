{"cells":[{"cell_type":"markdown","source":["Last test : 2021-01-30  \n","한국어 설명 : https://wikidocs.net/159246  \n","English Explanation : https://wikidocs.net/160289  \n","Github : https://github.com/RichardMinsooGo/51_Pretrained_BERT_NMT"],"metadata":{"id":"ioMc5WBYvl-j"}},{"cell_type":"markdown","source":["We wil use pytorch_pretrained_bert at this notebook"],"metadata":{"id":"Ecj68gnpo-zd"}},{"cell_type":"code","source":["!pip install pytorch_pretrained_bert\n","\n","from IPython.display import clear_output \n","clear_output()"],"metadata":{"id":"KYa0HIx6pDtF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the library that we will use. Then check whether GPU is selected.\n"],"metadata":{"id":"3oZlsQkypRWk"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils import data\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForQuestionAnswering, BertForPreTraining\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_glt0vUgpir0","executionInfo":{"status":"ok","timestamp":1643586884718,"user_tz":-540,"elapsed":5980,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"fa62bd07-86b9-4e8f-9ed2-27d8b4375ce7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["### 1. Data Load \n","This step is not used, we will define Input and Output at this note book.\n","\n","### 2. Build Input text, Output Text \n","Input/Output Data is defined."],"metadata":{"id":"RFp5NLY6rEG1"}},{"cell_type":"code","source":["raw_data = (\n","    ('What a ridiculous concept!', 'Quel concept ridicule !'),\n","    ('Your idea is not entirely crazy.', \"Votre idée n'est pas complètement folle.\"),\n","    (\"A man's worth lies in what he is.\", \"La valeur d'un homme réside dans ce qu'il est.\"),\n","    ('What he did is very wrong.', \"Ce qu'il a fait est très mal.\"),\n","    (\"All three of you need to do that.\", \"Vous avez besoin de faire cela, tous les trois.\"),\n","    (\"Are you giving me another chance?\", \"Me donnez-vous une autre chance ?\"),\n","    (\"Both Tom and Mary work as models.\", \"Tom et Mary travaillent tous les deux comme mannequins.\"),\n","    (\"Can I have a few minutes, please?\", \"Puis-je avoir quelques minutes, je vous prie ?\"))"],"metadata":{"id":"Sd22B8cxrE00"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Preprocess  \n","\n","Create spaces between words and punctuation marks.   \n","Ex) \"he is a boy.\" => \"he is a boy .\"   \n","Except (a-z, A-Z, \".\", \"?\", \"!\", \",\"), others are changed to space."],"metadata":{"id":"Msxnrv2AXmvr"}},{"cell_type":"code","source":["import unicodedata\n","import re\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","            if unicodedata.category(c) != 'Mn')\n","\n","def preprocess(sent):\n","    # 위에서 구현한 함수를 내부적으로 호출\n","    sent = unicode_to_ascii(sent.lower())\n","\n","    # 단어와 구두점 사이에 공백을 만듭니다.\n","    # Ex) \"he is a boy.\" => \"he is a boy .\"\n","    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n","    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","    sent = re.sub(r\"\\s+\", \" \", sent)\n","    return sent\n","\n","# 인코딩 테스트\n","en_sent = u\"Have you had dinner?\"\n","fr_sent = u\"Avez-vous déjà diné?\"\n","\n","print(preprocess(en_sent))\n","print(preprocess(fr_sent).encode('utf-8'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x19OZkORYB6U","executionInfo":{"status":"ok","timestamp":1643586887612,"user_tz":-540,"elapsed":2897,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"ad0d070e-7578-4bb7-cbb6-d89405a3c8a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["have you had dinner ?\n","b'avez vous deja dine ?'\n"]}]},{"cell_type":"markdown","source":["### Build Input/Output text data\n","In order to make input and output sentences into batch data, after preprocessing the raw data, convert it into a list and print it."],"metadata":{"id":"80M2o-kNIbrQ"}},{"cell_type":"code","source":["raw_encoder_input, raw_data_fr = list(zip(*raw_data))\n","raw_encoder_input, raw_data_fr = list(raw_encoder_input), list(raw_data_fr)\n","\n","input_text = ['[CLS] ' + preprocess(data) + ' [SEP]' for data in raw_encoder_input]\n","target_text = [preprocess(data) for data in raw_data_fr]\n","\n","print(input_text[:5])\n","print(target_text[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G9wM52obH4HA","executionInfo":{"status":"ok","timestamp":1643586887612,"user_tz":-540,"elapsed":4,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"a8b1c05a-1e8d-4c7a-cc51-7eed26fff45b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS] what a ridiculous concept ! [SEP]', '[CLS] your idea is not entirely crazy . [SEP]', '[CLS] a man s worth lies in what he is . [SEP]', '[CLS] what he did is very wrong . [SEP]', '[CLS] all three of you need to do that . [SEP]']\n","['quel concept ridicule !', 'votre idee n est pas completement folle .', 'la valeur d un homme reside dans ce qu il est .', 'ce qu il a fait est tres mal .', 'vous avez besoin de faire cela tous les trois .']\n"]}]},{"cell_type":"markdown","source":["### Load pretrained BERT Model\n","Load the predefined BERT model and check whether the input/output data is correctly created.  \n","In this article, the length of the input and output sentences is longer, and since tokens are divided into several tokens  when tokening is executed including French, the length of the input/output sequence is defined as 30."],"metadata":{"id":"bcXB09jjtGjk"}},{"cell_type":"code","source":["# Load pre-trained model tokenizer (vocabulary)\n","modelpath = \"bert-base-uncased\"\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","model = BertForMaskedLM.from_pretrained(modelpath)\n","model = model.to(device)\n","\n","n_seq_length = 80"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThSaEnzFCeQH","executionInfo":{"status":"ok","timestamp":1643586922240,"user_tz":-540,"elapsed":34630,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"b80e2acd-6a2a-4362-ceaa-f7faa41b9980"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 407873900/407873900 [00:15<00:00, 26641671.35B/s]\n"]}]},{"cell_type":"markdown","source":["### 4. Build Vocabulary\n","In the case of BertTokenizer, there is no need to create a dedicated vocabulary. It has its own built-in vocabulary, so you only need to define a tokenizer."],"metadata":{"id":"wdj99F4ay6UM"}},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(modelpath)"],"metadata":{"id":"DRy_ADWLy9Dk","executionInfo":{"status":"ok","timestamp":1643586923432,"user_tz":-540,"elapsed":1201,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"71918cd6-ccf1-4158-a0e4-630003144cc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 231508/231508 [00:00<00:00, 683299.41B/s]\n"]}]},{"cell_type":"markdown","source":["### 5. Tokenize \n","The tokenizing method is the same as the case of learning with only one sentence in the previous article. However, the difference is that, since it consists of several statements, the only difference is that each statement is executed using a function."],"metadata":{"id":"5Vbyn8CX3WsT"}},{"cell_type":"markdown","source":["### 6. Data Processing\n","In this article, \"6. Data Processing\" and \"7. Convert tokens to indexes\" are done simultaneously."],"metadata":{"id":"dcSJpP3RFUAe"}},{"cell_type":"markdown","source":["### 7. Convert tokens to indexes\n","As previously explained, this process is not in an exact order. It is the same as the case of learning with only one sentence in the previous article."],"metadata":{"id":"sBKh3crLFf_-"}},{"cell_type":"markdown","source":["### 8. Convert indexes to tensors \n","Convert the index created in Step 7 to tensors.  \n","Keep in mind that in deep learning, batch tensors are given as input.  \n","When creating input/output tokens with multiple statements, you need to create tensors that contain all of the data. The process is expressed as follows.  "],"metadata":{"id":"SjxfdhgLFhQm"}},{"cell_type":"code","source":["for idx in range(len(input_text)):\n","\n","    # 5. Tokenize\n","    tokenized_inp_text = tokenizer.tokenize(input_text[idx])\n","    tokenized_trg_text = tokenizer.tokenize(target_text[idx])\n","    len_input_text = len(tokenized_inp_text)\n","    \n","    # 6. Data Processing & 7. Convert tokens to indexes\n","    # Processing for model\n","    for _ in range(n_seq_length-len(tokenized_inp_text)):\n","        tokenized_inp_text.append('[MASK]')\n","\n","    indexed_inp_tokens = tokenizer.convert_tokens_to_ids(tokenized_inp_text)\n","\n","    pad_idx = -1\n","    converted_trg_inds = []\n","    converted_trg_inds = [pad_idx] * len_input_text\n","    \n","    indexed_trg_tokens = tokenizer.convert_tokens_to_ids(tokenized_trg_text)\n","    tmp_trg_tensors   = torch.tensor([indexed_trg_tokens])\n","    converted_trg_inds += tmp_trg_tensors[0].tolist()\n","    \n","    converted_trg_inds.append(tokenizer.convert_tokens_to_ids(['[SEP]'])[0])\n","\n","    for _ in range(n_seq_length-len(converted_trg_inds)):\n","        converted_trg_inds.append(pad_idx)\n","\n","    # 8. Convert indexes to tensors\n","    src_tensor = torch.tensor([indexed_inp_tokens]).to(device)\n","    trg_tensor = torch.tensor([converted_trg_inds]).to(device)\n","\n","    # When creating input/output tokens with multiple statements, you need to create tensors that contain all of the data. The process is expressed as follows.\n","    if idx == 0:\n","        tensors_src = src_tensor\n","    else :\n","        tensors_src = torch.cat((tensors_src, src_tensor), 0)\n","\n","    if idx == 0:\n","        tensors_trg = trg_tensor\n","    else :\n","        tensors_trg = torch.cat((tensors_trg, trg_tensor), 0)\n"],"metadata":{"id":"YXGphcoJDQpX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Others are normal training process"],"metadata":{"id":"08v0BokAmFd4"}},{"cell_type":"code","source":["# optimizer = torch.optim.Adam(model.parameters(), lr=5e-7)\n","# optimizer = torch.optim.SGD(model.parameters(), lr = 5e-5, momentum=0.9)\n","optimizer = torch.optim.Adamax(model.parameters(), lr = 5e-5)\n","\n","num_epochs = 300\n","\n","model.train()\n","for i in range(num_epochs):\n","    loss = model(tensors_src, masked_lm_labels=tensors_trg)\n","    eveloss = loss.mean().item()\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    if (i+1)%10 == 0:\n","        print(\"step \"+ str(i+1) + \" : \" + str(eveloss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hm-3siNo9M5D","executionInfo":{"status":"ok","timestamp":1643586960255,"user_tz":-540,"elapsed":36825,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"e622b8c2-4abc-4e91-c62c-cbf97e41e52b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 10 : 5.8896164894104\n","step 20 : 4.240797996520996\n","step 30 : 2.82885479927063\n","step 40 : 1.981243371963501\n","step 50 : 1.5004268884658813\n","step 60 : 1.0952112674713135\n","step 70 : 0.8964142799377441\n","step 80 : 0.6988181471824646\n","step 90 : 0.4597766697406769\n","step 100 : 0.2796086370944977\n","step 110 : 0.22384724020957947\n","step 120 : 0.21832644939422607\n","step 130 : 0.13917896151542664\n","step 140 : 0.09290452301502228\n","step 150 : 0.06827884912490845\n","step 160 : 0.057465214282274246\n","step 170 : 0.05416080355644226\n","step 180 : 0.04215862974524498\n","step 190 : 0.05267646908760071\n","step 200 : 0.05707816407084465\n","step 210 : 0.020082900300621986\n","step 220 : 0.02082022652029991\n","step 230 : 0.015502654016017914\n","step 240 : 0.012265943922102451\n","step 250 : 0.019831469282507896\n","step 260 : 0.010459261946380138\n","step 270 : 0.012032284401357174\n","step 280 : 0.00993704330176115\n","step 290 : 0.012004001066088676\n","step 300 : 0.007864445447921753\n"]}]},{"cell_type":"markdown","source":["### Inference\n","With the results learned in the previous process, select one of the data and test it. This process is the same as the previous article \"51.2 Single Sentence with BERT Tokenizer\" except for the sentence selection part."],"metadata":{"id":"AnGFtqvA9N0C"}},{"cell_type":"code","source":["print(tensors_src[6])\n","test_list = tensors_src[6].tolist()\n","test_tokens_tensor = torch.tensor([test_list]).to(device)\n","print(test_tokens_tensor)\n","\n","result = []\n","result_ids = []\n","model.eval()\n","with torch.no_grad():\n","    predictions = model(test_tokens_tensor)\n","\n","    start = len(tokenizer.tokenize(input_text[6]))\n","    count = 0\n","    while start < len(predictions[0]):\n","        predicted_index = torch.argmax(predictions[0,start]).item()\n","        \n","        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n","        if '[SEP]' in predicted_token:\n","            break\n","        if count == 0:\n","            result = predicted_token\n","            result_ids = [predicted_index]\n","        else:\n","            result+= predicted_token\n","            result_ids+= [predicted_index]\n","\n","        count += 1\n","        start += 1\n","print(\"input_text       :\", input_text[6])\n","print(\"target_text      :\", target_text[6])\n","print(\"tokenized target :\", tokenizer.tokenize(target_text[6]))\n","print(\"result_ids       :\",result_ids)\n","print(\"result           :\",result)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AMK_ZmolDR8-","executionInfo":{"status":"ok","timestamp":1643587063092,"user_tz":-540,"elapsed":347,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"87fc654f-92c7-4645-e96d-2a27fe27197f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 101, 2119, 3419, 1998, 2984, 2147, 2004, 4275, 1012,  102,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103], device='cuda:0')\n","tensor([[ 101, 2119, 3419, 1998, 2984, 2147, 2004, 4275, 1012,  102,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103]], device='cuda:0')\n","input_text       : [CLS] both tom and mary work as models . [SEP]\n","target_text      : tom et mary travaillent tous les deux comme mannequins .\n","tokenized target : ['tom', 'et', 'mary', 'tr', '##ava', '##ille', '##nt', 'to', '##us', 'les', 'deux', 'com', '##me', 'mann', '##e', '##quin', '##s', '.']\n","result_ids       : [3419, 3802, 2984, 19817, 12462, 10484, 3372, 2000, 2271, 4649, 24756, 4012, 4168, 10856, 2063, 12519, 2015, 1012]\n","result           : ['tom', 'et', 'mary', 'tr', '##ava', '##ille', '##nt', 'to', '##us', 'les', 'deux', 'com', '##me', 'mann', '##e', '##quin', '##s', '.']\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"51_3_Torch_Pretrained_BERT_NMT_8_sentence","provenance":[{"file_id":"1zZ7WtDX460crsmUO2Phs13ng22NIlucb","timestamp":1641807342023}],"authorship_tag":"ABX9TyM/IvXDG/erNoTg72OgJQ2Q"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}