{"cells":[{"cell_type":"markdown","source":["Last test : 2021-01-30  \n","한국어 설명 : https://wikidocs.net/159246  \n","English Explanation : https://wikidocs.net/160289  \n","Github : https://github.com/RichardMinsooGo/51_Pretrained_BERT_NMT"],"metadata":{"id":"DPMlgYlwp2fU"}},{"cell_type":"markdown","source":["We wil use pytorch_pretrained_bert at this notebook"],"metadata":{"id":"Ecj68gnpo-zd"}},{"cell_type":"code","source":["!pip install pytorch_pretrained_bert\n","\n","from IPython.display import clear_output \n","clear_output()"],"metadata":{"id":"KYa0HIx6pDtF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the library that we will use. Then check whether GPU is selected.\n"],"metadata":{"id":"3oZlsQkypRWk"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils import data\n","from pytorch_pretrained_bert import BertModel, BertForMaskedLM, BertForQuestionAnswering, BertForPreTraining\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_glt0vUgpir0","executionInfo":{"status":"ok","timestamp":1643528823686,"user_tz":-540,"elapsed":6442,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"35d879eb-1808-4c2f-d684-6c5cc83f797d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["### 1. Data Load \n","This step is not used, we will define Input and Output at this note book.\n","\n","### 2. Build Input text, Output Text \n","Input/Output Data is defined."],"metadata":{"id":"RFp5NLY6rEG1"}},{"cell_type":"code","source":["input_text  = \"I am a student\"\n","target_text = \"Je suis étudiant\""],"metadata":{"id":"Sd22B8cxrE00"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load pretrained BERT Model"],"metadata":{"id":"bcXB09jjtGjk"}},{"cell_type":"code","source":["modelpath = \"bert-base-uncased\"\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","model = BertForMaskedLM.from_pretrained(modelpath)\n","model = model.to(device)\n","\n","n_seq_length = 12\n","\n","print(\"input_text       :\", input_text)\n","print(\"target_text      :\", target_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nT_381tytL_M","executionInfo":{"status":"ok","timestamp":1643528852943,"user_tz":-540,"elapsed":29262,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"7281ef09-7e83-4c9b-df4b-cacba29b383c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 407873900/407873900 [00:11<00:00, 35385966.87B/s]\n"]},{"output_type":"stream","name":"stdout","text":["input_text       : I am a student\n","target_text      : Je suis étudiant\n"]}]},{"cell_type":"markdown","source":["### 3. Preprocess  \n","\n","Create spaces between words and punctuation marks.   \n","Ex) \"he is a boy.\" => \"he is a boy .\"   \n","Except (a-z, A-Z, \".\", \"?\", \"!\", \",\"), others are changed to space."],"metadata":{"id":"Msxnrv2AXmvr"}},{"cell_type":"code","source":["import unicodedata\n","import re\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","            if unicodedata.category(c) != 'Mn')\n","\n","def preprocess(sent):\n","    # Internal calling the function implemented above\n","    sent = unicode_to_ascii(sent.lower())\n","\n","    # Create spaces between words and punctuation marks.\n","    # Ex) \"he is a boy.\" => \"he is a boy .\"\n","    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","    # Except (a-z, A-Z, \".\", \"?\", \"!\", \",\"), others are changed to space.\n","    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","    sent = re.sub(r\"\\s+\", \" \", sent)\n","    return sent\n","\n","# Pre-Processing 테스트\n","en_sent = u\"Have you had dinner?\"\n","fr_sent = u\"Avez-vous déjà diné?\"\n","print(preprocess(en_sent))\n","print(preprocess(fr_sent).encode('utf-8'))\n","\n","\n","special_tokens = ['[CLS] [SEP] [MASK]']\n","\n","# ----------------------------------------------------------\n","input_text  = [preprocess(input_text)]\n","target_text = [preprocess(target_text)]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x19OZkORYB6U","executionInfo":{"status":"ok","timestamp":1643528854856,"user_tz":-540,"elapsed":1922,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"311a4a04-7e38-401d-dd91-b2505bdd371a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["have you had dinner ?\n","b'avez vous deja dine ?'\n"]}]},{"cell_type":"markdown","source":["### 4. Build Vocabulary\n","The example in this article uses \"Tokenizer\" that is defined in tensorflow as a tokenizer. In Pytorch, it was difficult to find a word level tokenizer, so I used tensorflow's one."],"metadata":{"id":"Z5KbMFv3bJYy"}},{"cell_type":"code","source":["# Encoder Input Define\n","tokenizer = Tokenizer(filters=\"\", lower=False)\n","tokenizer.fit_on_texts(special_tokens + input_text+ target_text)\n","\n","print(tokenizer.word_index)\n","print(tokenizer.index_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFf_QigHo8el","executionInfo":{"status":"ok","timestamp":1643528854856,"user_tz":-540,"elapsed":5,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"4a9cc2db-3e3e-416f-98cc-05e671f6ac6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'[CLS]': 1, '[SEP]': 2, '[MASK]': 3, 'i': 4, 'am': 5, 'a': 6, 'student': 7, 'je': 8, 'suis': 9, 'etudiant': 10}\n","{1: '[CLS]', 2: '[SEP]', 3: '[MASK]', 4: 'i', 5: 'am', 6: 'a', 7: 'student', 8: 'je', 9: 'suis', 10: 'etudiant'}\n"]}]},{"cell_type":"markdown","source":["### 5. Tokenize \n","### 7. Convert tokens to indexes\n","In this article, since only one sentence is trained, tokenizing is performed after adding [CLS] and [SEP] to the input and output sentences.\n","\n","In this article's example, \"Tokenize\" --> \"Convert tokens to indexes\" --> \"Data Processing\" was followed. That is, you can change the order according to the user's convenience and proceed."],"metadata":{"id":"4FnQNiQRdzX6"}},{"cell_type":"code","source":["input_text  = [(\"[CLS] \" + input_text[0]  + \" [SEP]\").split()]\n","target_text = [(target_text[0] + \" [SEP]\").split()]\n","\n","len_input_text = len(input_text[0])\n","\n","print(\"input_text       :\", input_text)\n","print(\"target_text      :\", target_text)\n","print(\"len_input_text   :\", len_input_text)\n","\n","tokenized_inp_text = tokenizer.texts_to_sequences(input_text)\n","tokenized_trg_text = tokenizer.texts_to_sequences(target_text)\n","\n","print(\"tokenized input  :\", tokenized_inp_text)\n","print(\"tokenized target :\", tokenized_trg_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ht6i-7SAdznK","executionInfo":{"status":"ok","timestamp":1643528854856,"user_tz":-540,"elapsed":4,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"eb57afe8-9b43-4c09-9b49-cc363f2d9012"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input_text       : [['[CLS]', 'i', 'am', 'a', 'student', '[SEP]']]\n","target_text      : [['je', 'suis', 'etudiant', '[SEP]']]\n","len_input_text   : 6\n","tokenized input  : [[1, 4, 5, 6, 7, 2]]\n","tokenized target : [[8, 9, 10, 2]]\n"]}]},{"cell_type":"markdown","source":["### 6. Data Processing\n","This process is configured differently according to BERT, Transformer, GPT, T5 and pretraining."],"metadata":{"id":"jBVSUA7pgy-J"}},{"cell_type":"code","source":["input_text = tokenizer.texts_to_sequences(input_text)\n","mask_idx   = tokenizer.texts_to_sequences(['[MASK]'])\n","indexed_inp_tokens = input_text[0] + mask_idx[0] * (n_seq_length - len_input_text)\n","\n","# use -1 or 0 only for pytorch_pretrained_bert\n","pad_idx = -1  \n","converted_trg_inds = []\n","converted_trg_inds = [pad_idx] * len_input_text\n","indexed_trg_tokens = tokenizer.texts_to_sequences(target_text)[0]\n","tmp_trg_tensors    = torch.tensor([indexed_trg_tokens])\n","converted_trg_inds += tmp_trg_tensors[0].tolist()\n","\n","for _ in range(n_seq_length-len(converted_trg_inds)):\n","    converted_trg_inds.append(pad_idx)\n","\n","print(\"Input (Tokenized and indexed)  :\\n\", indexed_inp_tokens)\n","print(\"Output (Tokenized and indexed) :\\n\", converted_trg_inds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bsmK9O_wgvlv","executionInfo":{"status":"ok","timestamp":1643528854856,"user_tz":-540,"elapsed":3,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"639d31a5-4d21-4f21-92ff-486a2c0fcaff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input (Tokenized and indexed)  :\n"," [1, 4, 5, 6, 7, 2, 3, 3, 3, 3, 3, 3]\n","Output (Tokenized and indexed) :\n"," [-1, -1, -1, -1, -1, -1, 8, 9, 10, 2, -1, -1]\n"]}]},{"cell_type":"markdown","source":["### 8. Convert indexes to tensors  \n","Convert the index created in Step 7 to tensors.\n","Keep in mind that in deep learning, batch + tensors are given as input."],"metadata":{"id":"nUVjTGfEksT_"}},{"cell_type":"code","source":["tensors_src = torch.tensor([indexed_inp_tokens]).to(device)\n","tensors_trg = torch.tensor([converted_trg_inds]).to(device)\n","print(tensors_src)\n","print(tensors_trg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cWvseT5Yl8hI","executionInfo":{"status":"ok","timestamp":1643528857175,"user_tz":-540,"elapsed":2321,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"136ea62c-99ab-4b82-e553-096e7c7754ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 4, 5, 6, 7, 2, 3, 3, 3, 3, 3, 3]], device='cuda:0')\n","tensor([[-1, -1, -1, -1, -1, -1,  8,  9, 10,  2, -1, -1]], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["### Others are normal training process"],"metadata":{"id":"08v0BokAmFd4"}},{"cell_type":"code","source":["# optimizer = torch.optim.Adam(model.parameters(), lr=5e-7)\n","# optimizer = torch.optim.SGD(model.parameters(), lr = 5e-5, momentum=0.9)\n","optimizer = torch.optim.Adamax(model.parameters(), lr = 5e-5)\n","\n","num_epochs = 1200\n","\n","model.train()\n","for i in range(num_epochs):\n","    loss = model(tensors_src, masked_lm_labels=tensors_trg)\n","    eveloss = loss.mean().item()\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    if (i+1)%10 == 0:\n","        print(\"step \"+ str(i+1) + \" : \" + str(eveloss))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6ly0_kSaeG6","executionInfo":{"status":"ok","timestamp":1643528930208,"user_tz":-540,"elapsed":73035,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"f137d0c5-a9f1-4dde-9ed3-bdcc3551e72f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 10 : 5.930408000946045\n","step 20 : 4.585104465484619\n","step 30 : 3.5422909259796143\n","step 40 : 2.747951030731201\n","step 50 : 2.2070822715759277\n","step 60 : 1.9274042844772339\n","step 70 : 1.7657920122146606\n","step 80 : 1.6707193851470947\n","step 90 : 1.599414587020874\n","step 100 : 1.5259640216827393\n","step 110 : 1.5084165334701538\n","step 120 : 1.5720945596694946\n","step 130 : 1.5188556909561157\n","step 140 : 1.4818816184997559\n","step 150 : 1.5042755603790283\n","step 160 : 1.4471206665039062\n","step 170 : 1.447527289390564\n","step 180 : 1.4522638320922852\n","step 190 : 1.4283106327056885\n","step 200 : 1.4422211647033691\n","step 210 : 1.4621987342834473\n","step 220 : 1.4405546188354492\n","step 230 : 1.4442800283432007\n","step 240 : 1.4225499629974365\n","step 250 : 1.3906773328781128\n","step 260 : 1.4254376888275146\n","step 270 : 1.4201207160949707\n","step 280 : 1.426276683807373\n","step 290 : 1.4335026741027832\n","step 300 : 1.4516173601150513\n","step 310 : 1.3820154666900635\n","step 320 : 1.4101217985153198\n","step 330 : 1.407749891281128\n","step 340 : 1.3969225883483887\n","step 350 : 1.433189868927002\n","step 360 : 1.4194440841674805\n","step 370 : 1.3654453754425049\n","step 380 : 1.431043028831482\n","step 390 : 1.4087679386138916\n","step 400 : 1.3949065208435059\n","step 410 : 1.4049077033996582\n","step 420 : 1.378011703491211\n","step 430 : 1.465254783630371\n","step 440 : 1.4180011749267578\n","step 450 : 1.3442671298980713\n","step 460 : 1.3763177394866943\n","step 470 : 1.4014997482299805\n","step 480 : 1.320591926574707\n","step 490 : 1.2103323936462402\n","step 500 : 1.007053256034851\n","step 510 : 0.8529963493347168\n","step 520 : 0.8212925791740417\n","step 530 : 0.8091986775398254\n","step 540 : 0.7642129063606262\n","step 550 : 0.7595958113670349\n","step 560 : 0.7354522943496704\n","step 570 : 0.7261370420455933\n","step 580 : 0.7011122703552246\n","step 590 : 0.6373803019523621\n","step 600 : 1.0977967977523804\n","step 610 : 0.6984370946884155\n","step 620 : 0.5155487060546875\n","step 630 : 0.3547059893608093\n","step 640 : 0.28658851981163025\n","step 650 : 0.3263717591762543\n","step 660 : 0.22741645574569702\n","step 670 : 0.2004234790802002\n","step 680 : 0.18367764353752136\n","step 690 : 0.25769516825675964\n","step 700 : 0.12655313313007355\n","step 710 : 0.2224423587322235\n","step 720 : 0.09631512314081192\n","step 730 : 0.09270836412906647\n","step 740 : 0.3969425857067108\n","step 750 : 0.08967152237892151\n","step 760 : 0.08712494373321533\n","step 770 : 0.10213324427604675\n","step 780 : 0.05551840737462044\n","step 790 : 0.06990323215723038\n","step 800 : 0.05552142113447189\n","step 810 : 0.10336097329854965\n","step 820 : 0.058965183794498444\n","step 830 : 0.07841382920742035\n","step 840 : 0.052988287061452866\n","step 850 : 0.07280293107032776\n","step 860 : 0.06351494044065475\n","step 870 : 0.042783163487911224\n","step 880 : 0.044171590358018875\n","step 890 : 0.03581690788269043\n","step 900 : 0.034697044640779495\n","step 910 : 0.04377542436122894\n","step 920 : 0.048767201602458954\n","step 930 : 0.03472784534096718\n","step 940 : 0.030997876077890396\n","step 950 : 0.03426093980669975\n","step 960 : 0.03287658840417862\n","step 970 : 0.02689504250884056\n","step 980 : 0.02673240564763546\n","step 990 : 0.029988795518875122\n","step 1000 : 0.6895560026168823\n","step 1010 : 0.053787920624017715\n","step 1020 : 0.03948817774653435\n","step 1030 : 0.03117223083972931\n","step 1040 : 0.030001558363437653\n","step 1050 : 0.028456129133701324\n","step 1060 : 0.0260775089263916\n","step 1070 : 0.1870335340499878\n","step 1080 : 0.13985292613506317\n","step 1090 : 0.05370670184493065\n","step 1100 : 0.03986009210348129\n","step 1110 : 0.028124473989009857\n","step 1120 : 0.02720307745039463\n","step 1130 : 0.024538036435842514\n","step 1140 : 0.025552235543727875\n","step 1150 : 0.020839814096689224\n","step 1160 : 0.019808227196335793\n","step 1170 : 0.022485680878162384\n","step 1180 : 0.022426962852478027\n","step 1190 : 0.019389446824789047\n","step 1200 : 0.015689419582486153\n","step 1210 : 0.016975944861769676\n","step 1220 : 0.018156781792640686\n","step 1230 : 0.016082197427749634\n","step 1240 : 0.01760929822921753\n","step 1250 : 0.015761390328407288\n","step 1260 : 0.015988102182745934\n","step 1270 : 0.014505424536764622\n","step 1280 : 0.01224520243704319\n","step 1290 : 0.014689966104924679\n","step 1300 : 0.01835009828209877\n","step 1310 : 0.014202641323208809\n","step 1320 : 0.012842588126659393\n","step 1330 : 0.012361296452581882\n","step 1340 : 0.013272986747324467\n","step 1350 : 0.012270662933588028\n","step 1360 : 0.011925122700631618\n","step 1370 : 0.016096986830234528\n","step 1380 : 0.011882631108164787\n","step 1390 : 0.011445599608123302\n","step 1400 : 0.010368108749389648\n","step 1410 : 0.010592740029096603\n","step 1420 : 0.011232681572437286\n","step 1430 : 0.01036002766340971\n","step 1440 : 0.010349871590733528\n","step 1450 : 0.011884743347764015\n","step 1460 : 0.01118314266204834\n","step 1470 : 0.010032284073531628\n","step 1480 : 0.01114111952483654\n","step 1490 : 0.010591108351945877\n","step 1500 : 0.009221652522683144\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"51_1_Torch_Pretrained_BERT_NMT_I_am_a_student","provenance":[],"authorship_tag":"ABX9TyMxSOPQpXJZC7Empkho+udc"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}