{"cells":[{"cell_type":"markdown","source":["Last test : 2021-01-30  \n","한국어 설명 : https://wikidocs.net/159246  \n","English Explanation : https://wikidocs.net/160289  \n","Github : https://github.com/RichardMinsooGo/51_Pretrained_BERT_NMT"],"metadata":{"id":"pfI15Sq7voTj"}},{"cell_type":"markdown","source":["We wil use pytorch_pretrained_bert at this notebook"],"metadata":{"id":"Ecj68gnpo-zd"}},{"cell_type":"code","source":["!pip install pytorch_pretrained_bert\n","\n","from IPython.display import clear_output \n","clear_output()"],"metadata":{"id":"KYa0HIx6pDtF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the library that we will use. Then check whether GPU is selected.\n"],"metadata":{"id":"3oZlsQkypRWk"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils import data\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForQuestionAnswering, BertForPreTraining\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_glt0vUgpir0","executionInfo":{"status":"ok","timestamp":1643587152542,"user_tz":-540,"elapsed":6089,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"169f8101-f7c6-4a21-d130-d2aca40d52e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["### 1. Data Load \n","This step is not used, we will define Input and Output at this note book.\n","\n","### 2. Build Input text, Output Text \n","Input/Output Data is defined."],"metadata":{"id":"RFp5NLY6rEG1"}},{"cell_type":"code","source":["raw_data = (\n","    ('What a ridiculous concept!', 'Quel concept ridicule !'),\n","    ('Your idea is not entirely crazy.', \"Votre idée n'est pas complètement folle.\"),\n","    (\"A man's worth lies in what he is.\", \"La valeur d'un homme réside dans ce qu'il est.\"),\n","    ('What he did is very wrong.', \"Ce qu'il a fait est très mal.\"),\n","    (\"All three of you need to do that.\", \"Vous avez besoin de faire cela, tous les trois.\"),\n","    (\"Are you giving me another chance?\", \"Me donnez-vous une autre chance ?\"),\n","    (\"Both Tom and Mary work as models.\", \"Tom et Mary travaillent tous les deux comme mannequins.\"),\n","    (\"Can I have a few minutes, please?\", \"Puis-je avoir quelques minutes, je vous prie ?\"),\n","\n","    (\"Could you close the door, please?\", \"Pourriez-vous fermer la porte, s'il vous plaît ?\"),\n","    (\"Did you plant pumpkins this year?\", \"Cette année, avez-vous planté des citrouilles ?\"),\n","    (\"Do you ever study in the library?\", \"Est-ce que vous étudiez à la bibliothèque des fois ?\"),\n","    (\"Don't be deceived by appearances.\", \"Ne vous laissez pas abuser par les apparences.\"),\n","    (\"Excuse me. Can you speak English?\", \"Je vous prie de m'excuser ! Savez-vous parler anglais ?\"),\n","    (\"Few people know the true meaning.\", \"Peu de gens savent ce que cela veut réellement dire.\"),\n","    (\"Germany produced many scientists.\", \"L'Allemagne a produit beaucoup de scientifiques.\"),\n","    (\"Guess whose birthday it is today.\", \"Devine de qui c'est l'anniversaire, aujourd'hui !\"),\n","    \n","    (\"He acted like he owned the place.\", \"Il s'est comporté comme s'il possédait l'endroit.\"),\n","    (\"Honesty will pay in the long run.\", \"L'honnêteté paye à la longue.\"),\n","    (\"How do we know this isn't a trap?\", \"Comment savez-vous qu'il ne s'agit pas d'un piège ?\"),\n","    (\"I can't believe you're giving up.\", \"Je n'arrive pas à croire que vous abandonniez.\"),\n","    (\"I have something very important to tell you.\", \"Il me faut vous dire quelque chose de très important.\"),\n","    (\"I have three times as many books as he does.\", \"J'ai trois fois plus de livres que lui.\"),\n","    (\"I have to change the batteries in the radio.\", \"Il faut que je change les piles de cette radio.\"),\n","    (\"I have to finish up some things before I go.\", \"Je dois finir deux trois trucs avant d'y aller.\"),\n","    \n","    (\"I have to think about what needs to be done.\", \"Je dois réfléchir sur ce qu'il faut faire.\"),\n","    (\"I haven't been back here since the incident.\", \"Je ne suis pas revenu ici depuis l'accident.\"),\n","    (\"I haven't eaten anything since this morning.\", \"Je n'ai rien mangé depuis ce matin.\"),\n","    (\"I hear his business is on the verge of ruin.\", \"Apparemment son entreprise est au bord de la faillite.\"),\n","    (\"I hope I didn't make you feel uncomfortable.\", \"J'espère que je ne t'ai pas mis mal à l'aise.\"),\n","    (\"I hope to continue to see more of the world.\", \"J'espère continuer à voir davantage le monde.\"),\n","    (\"I hope to see reindeer on my trip to Sweden.\", \"J'espère voir des rennes lors de mon voyage en Suède.\"),\n","    (\"I hope you'll find this office satisfactory.\", \"J'espère que ce bureau vous conviendra.\"),\n","\n","    (\"I hurried in order to catch the first train.\", \"Je me dépêchai pour avoir le premier train.\"),\n","    (\"I just can't stand this hot weather anymore.\", \"Je ne peux juste plus supporter cette chaleur.\"),\n","    (\"I just don't want there to be any bloodshed.\", \"Je ne veux tout simplement pas qu'il y ait une effusion de sang.\"),\n","    (\"I just thought that you wouldn't want to go.\", \"J'ai simplement pensé que vous ne voudriez pas y aller.\"),\n","    (\"I plan to go. I don't care if you do or not.\", \"Je prévois d'y aller. Ça m'est égal que vous y alliez aussi ou pas.\"),\n","    (\"I prefer soap as a liquid rather than a bar.\", \"Je préfère le savon liquide à une savonnette.\"),\n","    (\"I promise you I'll explain everything later.\", \"Je vous promets que j'expliquerai tout plus tard.\"),\n","    (\"I ran as fast as I could to catch the train.\", \"Je courus aussi vite que je pus pour attraper le train.\"))"],"metadata":{"id":"wlocjJz-O9_C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Preprocess  \n","\n","Create spaces between words and punctuation marks.   \n","Ex) \"he is a boy.\" => \"he is a boy .\"   \n","Except (a-z, A-Z, \".\", \"?\", \"!\", \",\"), others are changed to space."],"metadata":{"id":"Msxnrv2AXmvr"}},{"cell_type":"code","source":["import unicodedata\n","import re\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","            if unicodedata.category(c) != 'Mn')\n","\n","def preprocess(sent):\n","    # 위에서 구현한 함수를 내부적으로 호출\n","    sent = unicode_to_ascii(sent.lower())\n","\n","    # 단어와 구두점 사이에 공백을 만듭니다.\n","    # Ex) \"he is a boy.\" => \"he is a boy .\"\n","    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n","    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","    sent = re.sub(r\"\\s+\", \" \", sent)\n","    return sent\n","\n","# 인코딩 테스트\n","en_sent = u\"Have you had dinner?\"\n","fr_sent = u\"Avez-vous déjà diné?\"\n","\n","print(preprocess(en_sent))\n","print(preprocess(fr_sent).encode('utf-8'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x19OZkORYB6U","executionInfo":{"status":"ok","timestamp":1643587155295,"user_tz":-540,"elapsed":2756,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"5aa4f0da-a824-4bec-d17a-aeb74ee20776"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["have you had dinner ?\n","b'avez vous deja dine ?'\n"]}]},{"cell_type":"markdown","source":["### Build Input/Output text data\n","In order to make input and output sentences into batch data, after preprocessing the raw data, convert it into a list and print it."],"metadata":{"id":"80M2o-kNIbrQ"}},{"cell_type":"code","source":["raw_encoder_input, raw_data_fr = list(zip(*raw_data))\n","raw_encoder_input, raw_data_fr = list(raw_encoder_input), list(raw_data_fr)\n","\n","input_text = ['[CLS] ' + preprocess(data) + ' [SEP]' for data in raw_encoder_input]\n","target_text = [preprocess(data) for data in raw_data_fr]\n","\n","print(input_text[:5])\n","print(target_text[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G9wM52obH4HA","executionInfo":{"status":"ok","timestamp":1643587155295,"user_tz":-540,"elapsed":5,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"4d7ed307-e603-44ea-d811-a61cd8554c96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS] what a ridiculous concept ! [SEP]', '[CLS] your idea is not entirely crazy . [SEP]', '[CLS] a man s worth lies in what he is . [SEP]', '[CLS] what he did is very wrong . [SEP]', '[CLS] all three of you need to do that . [SEP]']\n","['quel concept ridicule !', 'votre idee n est pas completement folle .', 'la valeur d un homme reside dans ce qu il est .', 'ce qu il a fait est tres mal .', 'vous avez besoin de faire cela tous les trois .']\n"]}]},{"cell_type":"markdown","source":["### Load pretrained BERT Model\n","Load the predefined BERT model and check whether the input/output data is correctly created.  \n","In this article, the length of the input and output sentences is longer, and since tokens are divided into several tokens  when tokening is executed including French, the length of the input/output sequence is defined as 30."],"metadata":{"id":"bcXB09jjtGjk"}},{"cell_type":"code","source":["# Load pre-trained model tokenizer (vocabulary)\n","modelpath = \"bert-base-uncased\"\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","model = BertForMaskedLM.from_pretrained(modelpath)\n","model = model.to(device)\n","\n","n_seq_length = 80"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThSaEnzFCeQH","executionInfo":{"status":"ok","timestamp":1643587189087,"user_tz":-540,"elapsed":33795,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"bc7977a6-1ff5-46a5-970e-fc4535005047"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 407873900/407873900 [00:14<00:00, 27257026.68B/s]\n"]}]},{"cell_type":"markdown","source":["### 4. Build Vocabulary\n","In the case of BertTokenizer, there is no need to create a dedicated vocabulary. It has its own built-in vocabulary, so you only need to define a tokenizer."],"metadata":{"id":"wdj99F4ay6UM"}},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(modelpath)"],"metadata":{"id":"DRy_ADWLy9Dk","executionInfo":{"status":"ok","timestamp":1643587190333,"user_tz":-540,"elapsed":1256,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5d4699e-167f-4873-f40b-12b67dd686cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 231508/231508 [00:00<00:00, 692920.35B/s]\n"]}]},{"cell_type":"markdown","source":["### 5. Tokenize \n","The tokenizing method is the same as the case of learning with only one sentence in the previous article. However, the difference is that, since it consists of several statements, the only difference is that each statement is executed using a function."],"metadata":{"id":"5Vbyn8CX3WsT"}},{"cell_type":"markdown","source":["### 6. Data Processing\n","In this article, \"6. Data Processing\" and \"7. Convert tokens to indexes\" are done simultaneously."],"metadata":{"id":"dcSJpP3RFUAe"}},{"cell_type":"markdown","source":["### 7. Convert tokens to indexes\n","As previously explained, this process is not in an exact order. It is the same as the case of learning with only one sentence in the previous article."],"metadata":{"id":"sBKh3crLFf_-"}},{"cell_type":"markdown","source":["### 8. Convert indexes to tensors \n","Convert the index created in Step 7 to tensors.  \n","Keep in mind that in deep learning, batch tensors are given as input.  \n","When creating input/output tokens with multiple statements, you need to create tensors that contain all of the data. The process is expressed as follows.  "],"metadata":{"id":"SjxfdhgLFhQm"}},{"cell_type":"code","source":["for idx in range(len(input_text)):\n","\n","    # 5. Tokenize\n","    tokenized_inp_text = tokenizer.tokenize(input_text[idx])\n","    tokenized_trg_text = tokenizer.tokenize(target_text[idx])\n","    len_input_text = len(tokenized_inp_text)\n","    \n","    # 6. Data Processing & 7. Convert tokens to indexes\n","    # Processing for model\n","    for _ in range(n_seq_length-len(tokenized_inp_text)):\n","        tokenized_inp_text.append('[MASK]')\n","\n","    indexed_inp_tokens = tokenizer.convert_tokens_to_ids(tokenized_inp_text)\n","\n","    pad_idx = -1\n","    converted_trg_inds = []\n","    converted_trg_inds = [pad_idx] * len_input_text\n","    \n","    indexed_trg_tokens = tokenizer.convert_tokens_to_ids(tokenized_trg_text)\n","    tmp_trg_tensors   = torch.tensor([indexed_trg_tokens])\n","    converted_trg_inds += tmp_trg_tensors[0].tolist()\n","    \n","    converted_trg_inds.append(tokenizer.convert_tokens_to_ids(['[SEP]'])[0])\n","\n","    for _ in range(n_seq_length-len(converted_trg_inds)):\n","        converted_trg_inds.append(pad_idx)\n","\n","    # 8. Convert indexes to tensors\n","    src_tensor = torch.tensor([indexed_inp_tokens]).to(device)\n","    trg_tensor = torch.tensor([converted_trg_inds]).to(device)\n","\n","    # When creating input/output tokens with multiple statements, you need to create tensors that contain all of the data. The process is expressed as follows.\n","    if idx == 0:\n","        tensors_src = src_tensor\n","    else :\n","        tensors_src = torch.cat((tensors_src, src_tensor), 0)\n","\n","    if idx == 0:\n","        tensors_trg = trg_tensor\n","    else :\n","        tensors_trg = torch.cat((tensors_trg, trg_tensor), 0)\n"],"metadata":{"id":"YXGphcoJDQpX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9. Build batches \n","This is the part we will look at in detail in this article. The code below follows the general process of pytorch batch processing.\n","\n","As shown in the code below, dataset consists of source tensors and target tensors.\n","Dataloader can be configured simply by defining the dataset and batch size given above, and whether to use shuffle."],"metadata":{"id":"2JqEV8JOTryy"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset   # 텐서데이터셋\n","from torch.utils.data import DataLoader      # 데이터로더\n","\n","batch_size = 8\n","dataset = TensorDataset(tensors_src, tensors_trg)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"vwll576WTktT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Others are normal batch training process\n","The training process follows the general batch processing of pytorch.  \n","The process is almost identical to the previous articles, except for a few differences below.  \n","* Defines number of batches. It is used to calculate the loss per epoch.  \n","* Since there are n batches in the dataloader, learning is carried out by batch using \"for loop\".\n","* A process of finding the loss for each epoch is required."],"metadata":{"id":"08v0BokAmFd4"}},{"cell_type":"code","source":["# optimizer = torch.optim.Adam(model.parameters(), lr=5e-7)\n","# optimizer = torch.optim.SGD(model.parameters(), lr = 5e-5, momentum=0.9)\n","optimizer = torch.optim.Adamax(model.parameters(), lr = 5e-5)\n","\n","num_epochs = 300\n","\n","model.train()\n","# Defines number of batches. It is used to calculate the loss per epoch.\n","n_batches = len(dataset)/ batch_size\n","\n","for i in range(num_epochs):\n","    \n","    # Since there are n batches in the dataloader, learning is carried out by batch using \"for loop\".\n","    epoch_loss = 0\n","    for batch_idx, samples in enumerate(dataloader):\n","        x_train, y_train = samples\n","        loss = model(x_train, masked_lm_labels=y_train)\n","        eveloss = loss.mean().item()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # A process of finding the loss for each epoch is required.\n","        epoch_loss += eveloss / n_batches\n","\n","    if (i+1)%10 == 0:\n","        print(\"step \"+ str(i+1) + \" : \" + str(eveloss))\n","\n","print(tensors_src[6])\n","test_list = tensors_src[6].tolist()\n","test_tokens_tensor = torch.tensor([test_list]).to(device)\n","print(test_tokens_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzWxaKIMGdg3","executionInfo":{"status":"ok","timestamp":1643587373885,"user_tz":-540,"elapsed":183555,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"f4ad4d18-12d2-405b-ad52-8c5ae3077ff7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 10 : 4.809433460235596\n","step 20 : 3.385990619659424\n","step 30 : 2.146493673324585\n","step 40 : 1.6266932487487793\n","step 50 : 1.2720144987106323\n","step 60 : 0.9507423043251038\n","step 70 : 0.5502850413322449\n","step 80 : 0.4628187119960785\n","step 90 : 0.3671152591705322\n","step 100 : 0.15355569124221802\n","step 110 : 0.16645783185958862\n","step 120 : 0.1084912046790123\n","step 130 : 0.059116557240486145\n","step 140 : 0.06476368755102158\n","step 150 : 0.052303705364465714\n","step 160 : 0.035931188613176346\n","step 170 : 0.04314552620053291\n","step 180 : 0.024563908576965332\n","step 190 : 0.024123767390847206\n","step 200 : 0.0376523993909359\n","step 210 : 0.024672731757164\n","step 220 : 0.032974980771541595\n","step 230 : 0.015594548545777798\n","step 240 : 0.034659139811992645\n","step 250 : 0.00820515863597393\n","step 260 : 0.007579965982586145\n","step 270 : 0.005236814264208078\n","step 280 : 0.008152062073349953\n","step 290 : 0.010495786555111408\n","step 300 : 0.007919424213469028\n","tensor([ 101, 2119, 3419, 1998, 2984, 2147, 2004, 4275, 1012,  102,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103], device='cuda:0')\n","tensor([[ 101, 2119, 3419, 1998, 2984, 2147, 2004, 4275, 1012,  102,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103]], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["### Inference\n","With the results learned in the previous process, select one of the data and test it. This process is the same as the previous article \"51.2 Single Sentence with BERT Tokenizer\" except for the sentence selection part."],"metadata":{"id":"AnGFtqvA9N0C"}},{"cell_type":"code","source":["print(tensors_src[6])\n","test_list = tensors_src[6].tolist()\n","test_tokens_tensor = torch.tensor([test_list]).to(device)\n","print(test_tokens_tensor)\n","\n","result = []\n","result_ids = []\n","model.eval()\n","with torch.no_grad():\n","    predictions = model(test_tokens_tensor)\n","\n","    start = len(tokenizer.tokenize(input_text[6]))\n","    count = 0\n","    while start < len(predictions[0]):\n","        predicted_index = torch.argmax(predictions[0,start]).item()\n","        \n","        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n","        if '[SEP]' in predicted_token:\n","            break\n","        if count == 0:\n","            result = predicted_token\n","            result_ids = [predicted_index]\n","        else:\n","            result+= predicted_token\n","            result_ids+= [predicted_index]\n","\n","        count += 1\n","        start += 1\n","print(\"input_text       :\", input_text[6])\n","print(\"target_text      :\", target_text[6])\n","print(\"tokenized target :\", tokenizer.tokenize(target_text[6]))\n","print(\"result_ids       :\",result_ids)\n","print(\"result           :\",result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AMK_ZmolDR8-","executionInfo":{"status":"ok","timestamp":1643587373886,"user_tz":-540,"elapsed":9,"user":{"displayName":"Richard Minsoo Go","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09628967547080180869"}},"outputId":"db58251e-1f96-40e6-8426-c7b5566c6173"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 101, 2119, 3419, 1998, 2984, 2147, 2004, 4275, 1012,  102,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","         103,  103,  103,  103,  103,  103,  103,  103], device='cuda:0')\n","tensor([[ 101, 2119, 3419, 1998, 2984, 2147, 2004, 4275, 1012,  102,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n","          103,  103,  103,  103,  103,  103,  103,  103]], device='cuda:0')\n","input_text       : [CLS] both tom and mary work as models . [SEP]\n","target_text      : tom et mary travaillent tous les deux comme mannequins .\n","tokenized target : ['tom', 'et', 'mary', 'tr', '##ava', '##ille', '##nt', 'to', '##us', 'les', 'deux', 'com', '##me', 'mann', '##e', '##quin', '##s', '.']\n","result_ids       : [3419, 3802, 2984, 19817, 12462, 10484, 3372, 2000, 2271, 4649, 24756, 4012, 4168, 10856, 2063, 12519, 2015, 1012]\n","result           : ['tom', 'et', 'mary', 'tr', '##ava', '##ille', '##nt', 'to', '##us', 'les', 'deux', 'com', '##me', 'mann', '##e', '##quin', '##s', '.']\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"51_4_Torch_Pretrained_BERT_NMT_40_sentence","provenance":[{"file_id":"1u54wOcPvmnVJOrWE038l8aAS5WydSC_l","timestamp":1641878659907},{"file_id":"1zZ7WtDX460crsmUO2Phs13ng22NIlucb","timestamp":1641807342023}],"authorship_tag":"ABX9TyPz9MpaepsfPIP4g3xq4qvw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}